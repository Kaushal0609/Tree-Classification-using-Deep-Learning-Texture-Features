{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a50ff-3735-466c-a2c5-6ff06ff53262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops, hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import shannon_entropy\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ============ IMAGE ENHANCEMENT FUNCTION ============\n",
    "def enhance_image(image):\n",
    "    \"\"\"Applies CLAHE to enhance image contrast.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))  # Increased clipLimit\n",
    "    return clahe.apply(gray)\n",
    "\n",
    "# ============ EXTRACT TEXTURE FEATURES FUNCTION ============\n",
    "def extract_texture_features(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "\n",
    "    gray = rgb2gray(image)  \n",
    "    gray = (gray * 255).astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "    # Enhancement\n",
    "    enhanced_gray = enhance_image(image)\n",
    "\n",
    "    # Gabor Features\n",
    "    gabor_kernels = [cv2.getGaborKernel((5, 5), sigma, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "                     for sigma in [1, 3] for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]]\n",
    "    gabor_features = [np.mean(cv2.filter2D(enhanced_gray, cv2.CV_8UC3, k)) for k in gabor_kernels]\n",
    "\n",
    "    # GLCM Features\n",
    "    glcm = graycomatrix(enhanced_gray, distances=[1, 2], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], symmetric=True, normed=True)\n",
    "    glcm_features = [graycoprops(glcm, prop).mean() for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']]\n",
    "\n",
    "    # LBP Features\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(11), density=True)\n",
    "\n",
    "    # HOG Features\n",
    "    hog_features = hog(gray, orientations=6, pixels_per_cell=(8, 8), cells_per_block=(1, 1), feature_vector=True)\n",
    "\n",
    "    # Statistical Features\n",
    "    ent = shannon_entropy(enhanced_gray)\n",
    "    mean, variance, skewness, kurtosis = np.mean(enhanced_gray), np.var(enhanced_gray), stats.skew(enhanced_gray.flatten()), stats.kurtosis(enhanced_gray.flatten())\n",
    "\n",
    "    # SIFT Features\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, _ = sift.detectAndCompute(enhanced_gray, None)\n",
    "    sift_count = len(keypoints)\n",
    "\n",
    "    # Combine all features\n",
    "    combined_features = np.hstack((glcm_features, lbp_hist, hog_features, gabor_features, [ent, mean, variance, skewness, kurtosis, sift_count]))\n",
    "    return combined_features\n",
    "\n",
    "# ============ LOAD DATASET FUNCTION ============\n",
    "def load_dataset(split):\n",
    "    tree_images = tf.io.gfile.glob(f\"dataset/{split}/tree/*\")\n",
    "    non_tree_images = tf.io.gfile.glob(f\"dataset/{split}/non_tree/*\")\n",
    "\n",
    "    image_paths = tree_images + non_tree_images\n",
    "    labels = [1] * len(tree_images) + [0] * len(non_tree_images)\n",
    "\n",
    "    # Extract features\n",
    "    texture_features = [extract_texture_features(img) for img in image_paths]\n",
    "\n",
    "    return np.array(image_paths), np.array(labels, dtype=np.float32), np.array(texture_features, dtype=np.float32)\n",
    "\n",
    "# ============ CUSTOM DATA GENERATOR ============\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_generator, manual_features, batch_size):\n",
    "        self.image_generator = image_generator\n",
    "        self.manual_features = manual_features\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_generator)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_batch, label_batch = self.image_generator[index]\n",
    "\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = (index + 1) * self.batch_size\n",
    "        manual_batch = self.manual_features[start_idx:end_idx]\n",
    "\n",
    "        manual_batch_tensor = tf.convert_to_tensor(manual_batch, dtype=tf.float32)\n",
    "        image_batch_tensor = tf.convert_to_tensor(image_batch, dtype=tf.float32)\n",
    "\n",
    "        return (image_batch_tensor, manual_batch_tensor), label_batch\n",
    "\n",
    "# ============ BUILD MODEL ============\n",
    "def build_model():\n",
    "    image_input = layers.Input(shape=(128, 128, 3), name=\"image_input\")\n",
    "    base_cnn = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_tensor=image_input)\n",
    "\n",
    "    for layer in base_cnn.layers[-10:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    cnn_output = layers.GlobalAveragePooling2D()(base_cnn.output)\n",
    "\n",
    "    feature_input = layers.Input(shape=(1566,), name=\"feature_input\")  \n",
    "    feature_branch = layers.Dense(128, activation='relu')(feature_input)\n",
    "    feature_branch = layers.Dense(64, activation='relu')(feature_branch)\n",
    "\n",
    "    merged = layers.Concatenate()([cnn_output, feature_branch])\n",
    "    attention = layers.Dense(256, activation='relu')(merged)\n",
    "    output = layers.Dense(1, activation='sigmoid')(attention)\n",
    "\n",
    "    model = models.Model(inputs=[image_input, feature_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ============ LOAD DATA ============\n",
    "train_paths, train_labels, train_features = load_dataset('train')\n",
    "val_paths, val_labels, val_features = load_dataset('val')\n",
    "\n",
    "train_labels = train_labels.astype(str)\n",
    "val_labels = val_labels.astype(str)\n",
    "\n",
    "train_labels = train_labels.astype(float).astype(int)\n",
    "\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ").flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({\"filename\": train_paths, \"class\": train_labels.astype(str)}),  # Convert labels to string\n",
    "    x_col=\"filename\", y_col=\"class\", target_size=(128, 128), batch_size=32, class_mode='binary'\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({\"filename\": val_paths, \"class\": val_labels.astype(str)}),  # Convert labels to string\n",
    "    x_col=\"filename\", y_col=\"class\", target_size=(128, 128), batch_size=32, class_mode='binary'\n",
    ")\n",
    "\n",
    "dataset_train = CustomDataGenerator(train_gen, train_features, batch_size=32)\n",
    "dataset_val = CustomDataGenerator(val_gen, val_features, batch_size=32)\n",
    "\n",
    "# ============ TRAINING ============\n",
    "model = build_model()\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(2)}\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    validation_data=dataset_val,\n",
    "    epochs=8,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "model.save(\"tree_detection_model.h5\")  # Saves the model as a .h5 file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
